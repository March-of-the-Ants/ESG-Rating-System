{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbb8a7c",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 기업리스트 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8475944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9966147",
   "metadata": {},
   "source": [
    "## 2. 잡플래닛 리뷰 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea28e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"k1g3Zx8nVTbbRmXRx3qS\"\n",
    "client_secret = \"wzwAGiJ2md\"\n",
    "headers = {'user-agent': 'Mozilla/5.0'} \n",
    "\n",
    "news = pd.DataFrame(columns=('사업자등록번호', '뉴스링크'))\n",
    "company_list, link_list = [], []\n",
    "for idx in tqdm(range(len(kosdaq))):  \n",
    "\n",
    "\n",
    "\n",
    "login_url = 'https://www.jobplanet.co.kr/users/sign_in'\n",
    "\n",
    "#email 본인 아이디, password 본인 패스워드 입력 단, 리뷰를 남겨서 전체 접근이 가능한 상태여야함\n",
    "#현재 크롤링 코드에서는 아이디 비밀번호 입력 제외\n",
    "email = ''\n",
    "password = ''\n",
    "\n",
    "\n",
    "LOGIN_INFO ={\n",
    "    'user[email]' : email,\n",
    "    'user[password]' : password,\n",
    "    'commit' : '로그인'\n",
    "}\n",
    "\n",
    "session = requests.session()\n",
    "\n",
    "res = session.post(login_url, data = LOGIN_INFO, verify = False)\n",
    "\n",
    "res.raise_for_status()\n",
    "result = []\n",
    "\n",
    "\n",
    "def clean_str(text):\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    text = text.replace('\\r','. ')\n",
    "    return text\n",
    "\n",
    "def get_number(text):\n",
    "  pattern =  \"/([0-9]+)/\"\n",
    "  result = re.search(pattern, text)\n",
    "  return result.group(1)\n",
    "\n",
    "queries = queries_CMP_NM_PFIX_SFIX\n",
    "\n",
    "for n,query in enumerate(queries):\n",
    "  time.sleep(3)\n",
    "  url = 'https://www.jobplanet.co.kr/search/companies/'+str(query)\n",
    "  #urls 만들기\n",
    "  #bs로 해당 서치에 query와 똑같은 text의 주소 저장 => urls로 보내기\n",
    "  res = session.get(url)\n",
    "  try:\n",
    "    res.raise_for_status()\n",
    "  except: continue\n",
    "  soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "  url_set = [i.a['href'] for i in soup.find_all('dt','us_titb_l3') if i.a.text in query]\n",
    "\n",
    "  res_set =[]\n",
    "  urls =[]\n",
    "\n",
    "  for i in url_set:\n",
    "    urls.append('https://www.jobplanet.co.kr/'+i)\n",
    "\n",
    "  for i in urls:\n",
    "    res_set.append(session.get(i).url)\n",
    "\n",
    "  for j in res_set:\n",
    "  #url 은 보고싶은 기업의 리뷰 URL 이며 마지막은 ?page= 형태로 해야함, last_page는 해당 기업 리뷰의 마지막 페이지 입력\n",
    "    last_page = 6\n",
    "    time.sleep(3)\n",
    "    for idx in range(1,last_page):\n",
    "        url = j+'?page='+str(idx)\n",
    "        res = session.get(url)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        count3 = 0\n",
    "        count4 = 0\n",
    "        count5 = 0\n",
    "\n",
    "        try:\n",
    "            for k in range(5):\n",
    "                reviewer_info = []\n",
    "                # 응답자 정보\n",
    "                position = soup.select('.content_top_ty2 > span.txt1')[0 + count4].text\n",
    "                status = soup.select('.content_top_ty2 > span.txt1')[1 + count4].text\n",
    "                loc = soup.select('.content_top_ty2 > span.txt1')[2 + count4].text\n",
    "\n",
    "                day = soup.select('.content_top_ty2 > span.txt1')[3+ count4].text\n",
    "\n",
    "                # 점\n",
    "                star_rating = soup.select('.us_star_m > div.star_score')[0+k]['style'][6:-1]\n",
    "\n",
    "                # rating 5*5\n",
    "                promotion = soup.select('.bl_score')[0 + count5]['style'][6:-1]\n",
    "                welfare = soup.select('.bl_score')[1 + count5]['style'][6:-1]\n",
    "                balance = soup.select('.bl_score')[2 + count5]['style'][6:-1]\n",
    "                culture = soup.select('.bl_score')[3 + count5]['style'][6:-1]\n",
    "                top = soup.select('.bl_score')[4 + count5]['style'][6:-1]\n",
    "                # 중심 제목\n",
    "                content = soup.select('h2.us_label')[0+k].text\n",
    "                # 장단점 경영진 의견\n",
    "                merit = soup.select('dl.tc_list > dd.df1 > span')[0 + count3].text\n",
    "                disadvantages = soup.select('dl.tc_list > dd.df1 > span')[1 + count3].text\n",
    "                df_tit = soup.select('dl.tc_list > dd.df1 > span')[2 + count3].text\n",
    "\n",
    "\n",
    "                reviewer_info = [position, status, loc, day, star_rating, promotion, welfare, balance, culture, top,clean_str(content),clean_str(merit),clean_str(disadvantages), clean_str(df_tit),get_number(url),query]\n",
    "\n",
    "                result.append(reviewer_info)\n",
    "                reviewer_info=[]\n",
    "                count3 += 3\n",
    "                count4 += 4\n",
    "                count5 += 5\n",
    "                # print(\"pass :\"+str(idx)+\"-\"+str(k))\n",
    "        except :\n",
    "            print(\"fail :\" + str(idx))\n",
    "            pass\n",
    "\n",
    "    colname = ['직무','상황','지역','작성일','총점','승진 기회 및 가능성','복지 및 급여','업무와 삶의 균형','사내문화','경영진','총평','장점','단점','바라는점','잡플래닛 company Id','회사명']\n",
    "    df = pd.DataFrame(result,columns=colname)\n",
    "    #200개의 회사 마다 checkpoint 생성\n",
    "    if n%200==0:\n",
    "      print(\"###################SAVE CHPT####################\")\n",
    "      df.to_excel(\"jp_actv_pre\"+str(n)+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
